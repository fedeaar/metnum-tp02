% === INTRO === %

\vspace{1em}
\subsection{Experimentación Método de la potencia} 

\begin{large}  
    Ya vimos que si los autovalores son todos diferentes en módulo el método funciona bien. \textbf{¿Qué es lo que pasa cuando hay autovalores de igual módulo?}
\end{large}   


\vspace{2em}
\noindent \textsc{Método de la potencia para autovalores dominantes repetidos}

\vspace{2em}
\noindent Para simplificar el análisis separamos en casos: \\
\indent 1) Autovalores dominantes iguales \\
\indent 2) Autovalores dominantes que tienen el mismo valor absoluto
\vspace{2em}



\noindent \textsc{1) Método de la potencia para autovalores dominantes iguales}
\begin{align}
    let \ norm \ :&= \ ||\sum_{i=1}^{n} a_i \ \lambda_{i}^{k} \ x_i||_2 \\
    \lim_{k \to \infty} y_k &= \frac{\sum_{i=1}^{n} a_i \ \lambda_{i}^{k} \ x_i }{norm} \\ 
    \lim_{k \to \infty} y_k &= \frac{(\sum_{i=1}^{n} a_i \ \lambda_{i}^{k} \ x_i) / \lambda_{1}^{k}}{norm / \lambda_{1}^{k}} \\
    let \ b_i \ :&= \ sign(\lambda_{1}^{k}) \frac{a_i}{norm / |\lambda_{1}^{k}|} \\
    \lim_{k \to \infty} y_k &= \sum_{i=1}^{n} b_i \ \frac{\lambda_{i}^{k}}{\lambda_{1}^{k}} \ x_i \\
    \lim_{k \to \infty} y_k &= b_1 x_1 + b_2 x_2 
\end{align}

\vspace{1em}
\noindent Como $x_1$ y $x_2$ son autovectores con el mismo $\lambda$ entonces:
\begin{align}
    dados\ c_1 \ y \ c_2 &\in \mathbb{R} \\
    A \ (c_1 \ x_1 + c_2 \ x_2) &= A c_1 x_1 + A c_2 x_2 \\
    A \ (c_1 \ x_1 + c_2 \ x_2) &= \lambda c_1 x_1 + \lambda c_2 x_2 \\
    A \ (c_1 \ x_1 + c_2 \ x_2) &= \lambda (c_1 x_1 + c_2 x_2) 
\end{align}

Lo que se puede concluir de este resultado es que no importa quienes sean $c_1$, $c_2$ se cumple que ($c_1 x_1 + c_2 x_2$) es un autovector y $\lambda$ es su autovalor por lo tanto en el caso donde hay 2 autovalores dominantes iguales, el método de la potencia converge correctamente este resultado lo comprobamos experimentalmente como se ve en el gráfico (\ref{fig:autovalor_repetido}), donde se observa que el autovector inicial tiende a uno cuyo autovalor es correcto a medida que aumenta la cantidad de iteraciones del algoritmo. Además el resultado es generalizable, siguiendo pasos similares es fácil ver que converge no solo cuando la cantidad de autovalores dominantes repetidos son 2 sino que no importa la cantidad de veces que esté repetido el autovalor dominante se cumple que $\sum_{i=1}^{n} (a_i \ \lambda_{i}^{k} \ x_i)$ resulta en un autovector de A.

\vspace{1em}

Cabe destacar que como $(c_1 x_1 + c_2 x_2)$ es autovector $\forall c_1, \ c_2$ es lo mismo que decir que cualquier combinación lineal de $x_1$ y $x_2$ va a ser autovector por ende hay infinitas posibilidades para los 2 autovectores dominantes. A diferencia del caso en el que todos los autovalores son diferentes en módulo donde solo hay dos autovectores con norma 2 igual a 1, ahora las posibilidades son infinitas, por lo tanto hay que tener esto en cuenta a la hora de realizar comparaciones con los resultados que obtiene numpy. 

\vspace{1em}


Algo interesante es que si $\lambda$ es negativo, recordando que $b_i = sign(\lambda^{k}) \ \frac{a_i}{norm / |\lambda^{k}|}$, además declaramos $d_i$ = $\frac{a_i}{norm / |\lambda^{k}|}$. Lo que observamos es que en las iteraciones pares $sign(\lambda^{k}) = 1$, por lo tanto, $(\lim_{k \to \infty} y_k = d_1 x_1 + d_2 x_2)$ pero en las impares $sign(\lambda^{k}) = -1$, por lo tanto, $(\lim_{k \to \infty} y_k = - \ d_1 x_1 -d_2 x_2)$, ambos son autovectores válidos linealmente dependientes. Teniendo en cuenta que el método depende de la distancia entre una iteración y su consecutiva, como en este caso el método oscila entre iteraciones pares e iteraciones impares la distancia entre una iteración y su consecutiva no tiende a 0, por lo que el método va a tener que que hacer el total de las iteraciones sin poder interrumpirse. De esta observación concluimos que es conveniente hacer el método de la potencia de a 2 pasos por vez, pero debería converger correctamente a un autovector sin ninguna modificación.

\vspace{1em}
\begin{figure}[!htbp]
    \includegraphics[scale=0.45]{files/src/.media/op_autovalor_repetido.png}
    \caption{Se eligió una matriz de 20x20 con base de autovectores y 2 autovalores dominantes iguales a 20. En cada iteración del método de la potencia almacenamos $|20 - autovalor|$. Se puede observar claramente como a medida que aumenta la cantidad de iteraciones la distancia entre el autovalor y el esperado tiende a 0.}
    \label{fig:autovalor_repetido}
\end{figure}

\noindent \textsc{2) Método de la potencia para autovalores diferentes pero iguales en módulo}
\begin{align}
    let \ norm \ :&= \ ||\sum_{i=1}^{n} a_i \ \lambda_{i}^{k} \ x_i||_2 \\
    \lim_{k \to \infty} y_k &= \frac{\sum_{i=1}^{n} a_i \ \lambda_{i}^{k} \ x_i }{norm} \\ 
    \lim_{k \to \infty} y_k &= \frac{(\sum_{i=1}^{n} a_i \ \lambda_{i}^{k} \ x_i) / \lambda_{1}^{k}}{norm / \lambda_{1}^{k}} \\
    let \ b_i \ :&= \ \frac{a_i}{norm / \lambda_{1}^{k}} \\
    \lim_{k \to \infty} y_k &= \sum_{i=1}^{n} b_i \ \frac{\lambda_{i}^{k}}{\lambda_{1}^{k}} \ x_i \\
    \lim_{k \to \infty} y_k &= b_1 x_1 + b_2 \ \frac{\lambda_{2}^{k}}{\lambda_{1}^{k}} \ x_2 \\
    \lim_{k \to \infty} y_k &= b_1 x_1 + b_2 \ sign(\lambda_{2}^{k}) \ x_2 
\end{align}


\noindent Notamos que este vector también oscila entre k par y k impar, ya que si k es par: \\
\indent\indent $\lim_{k \to \infty} y_k = b_1 x_1 + b_2 x_2$ \\ \\
Pero si k es impar: \\
\indent \indent $\lim_{k \to \infty} y_k = b_1 x_1 - b_2 x_2$ \\ \\
Se ve claramente que si $b_2 x_2$ no es 0 son dos vectores diferentes.
Pero para mayor seguridad lo comprobamos experimentalmente, como se puede observar en el gráfico (\ref{fig:oscilante}).

\begin{figure}[!htbp]
    \includegraphics[scale=0.45]{files/src/.media/op_oscilante.png}
    \caption{Se eligió una matriz de 20x20 con base de autovectores y 2 autovalores dominantes 20, -20. Además se seleccionó un vector aleatorio constante normalizado y en cada iteración del método de la potencia calculamos $||randVector - y_k||_2$. Se puede apreciar claramente como la distancia al randVector oscila entre las iteraciones pares e impares.}
    \label{fig:oscilante}
\end{figure}

\vspace{2em}

Queremos ver si alguno de los vectores a los que converge $y_k$ es autovector de A:
\begin{align}
    dados\ c_1,& \ c_2 \in \mathbb{R} \\
    A \ (c_1 x_1 + c_2 x_2) &= A c_1  x_1 + A c_2 x_2 \\
    A \ (c_1 x_1 + c_2 x_2) &= \lambda c_1  x_1 - \lambda c_2 x_2 \\
    A \ (c_1 x_1 + c_2 x_2) &= \lambda (c_1 x_1 - c_2 x_2) 
\end{align}

Lo que se puede concluir de este resultado es que si $c_1$ y $c_2$ son ambos desiguales a 0 entonces $(c_1 x_1 + c_2 x_2)$ no es autovector. Teniendo en cuenta que $b_i \ := \ \frac{a_i}{norm / \lambda_{1}^{k}}$ y que $a_1$, $a_2$ pueden no ser 0, podemos afirmar con seguridad que si hay un autovalor positivo y un autovalor negativo dominantes, entonces $\lim_{k \to \infty} y_k$ por lo mencionado previamente NO es autovector de la matriz inicial.

\vspace{1em}

Con este resultado en mente observamos que en caso de computar la suma entre las últimas dos iteraciones del método de la potencia obtendríamos lo siguiente:

\vspace{1em}
Dado k par suficientemente grande para que el método converja: 
\begin{align}
    y_k &= b_1 x_1 + b_2 x_2 \\ 
    y_{k+1} &= b_1 x_1 - b_2 x_2 \\ 
    y_k + y_{k+1} &= b_1 x_1 + b_2 x_2 + b_1 x_1 - b_2 x_2 \\ 
    y_k + y_{k+1} &= 2 b_1 x_1
\end{align}

Sabemos que $x_1$ es autovector, por ende $2 b_1 x_1$ también es autovector.

\vspace{1em}

Comentario: Observamos que se podría calcular la resta para obtener el otro autovector($x_2$) pero la deflacíon asume que el método de la potencia encuentra de a 1 autovector por llamado por lo que habría que cambiar varios algoritmos para poder aprovechar este caso puntual y consideramos que no vale la pena.

\vspace{1em}

Siguiendo pasos similares a los que venimos realizando se puede demostrar que la suma de dos iteraciones consecutivas resulta en un autovector de la matriz inicial se mantiene sin importar cuantas veces este repetido el autovalor positivo dominante o cuantas veces esté repetido el autovalor negativo dominante, siempre y cuando se cumpla que hay al menos 1 de cada uno.

\vspace{1em}
Con este resultado en mente planteamos una alternativa al método de la potencia de la siguiente forma:

\vspace{1em}
\lstinputlisting[mathescape=true, escapechar=@, language=pseudo, label=potencia_modificada, caption={Pseudocódigo mostrando la alternativa a la potencia que soluciona cuando hay autovalores repetidos}]{files/src/.code/potencia_modificada.pseudo}

\vspace{1em}
Dada una matriz inicial cuyos autovalores pertenecen a los reales, con este nuevo algoritmo sumado a la deflación original sin modificaciones se pueden obtener todos los autovectores y autovalores de la matriz.

\vspace{1em}

\begin{large}  
    \textbf{¿Cuantás iteraciones del método de la potencia son necesarias para que converja según el tamaño de la matriz?}
\end{large}  

\vspace{1em}
Para responder esta pregunta experimentamos de la siguiente manera:

\vspace{1em}
\lstinputlisting[language=pseudo, label=experimento_iteraciones, caption={Experimento para entender la complejidad del método de la potencia}]{files/src/.code/experimento_iteraciones.pseudo}
\vspace{1em}

\vspace{1em}
\begin{figure}[!htbp]
    \includegraphics[scale=0.45]{files/src/.media/op_experimento_iteraciones.png}
    \caption{Por cada i del algoritmo mostrado arriba se calculó el promedio de las iteraciones y la cantidad máxima de iteraciones.}
    \label{fig:iteraciones}
\end{figure}
\vspace{1em}

Se puede apreciar en el gráfico (\ref{fig:iteraciones}) que ambas curvas asemejan ser funciones lineales, deducimos que la cantidad de iteraciones necesarias para que el método de la potencia converja es lineal con respecto al tamaño de la matriz inicial.
Este resultado, sumado a que la complejidad del método de la potencia es $O(N^2 * iteraciones)$ nos permite concluir que la complejidad promedio del método de la potencia es $\Theta(N^3)$